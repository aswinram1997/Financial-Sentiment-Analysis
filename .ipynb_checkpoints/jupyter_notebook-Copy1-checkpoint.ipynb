{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca00a03",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77efc3",
   "metadata": {},
   "source": [
    "In today's data-driven financial landscape, the development of a Financial Sentiment Analysis Model is of paramount importance. This model harnesses natural language processing and machine learning techniques to systematically evaluate sentiment in financial data, offering invaluable insights for investment decisions, risk management, and market forecasting. By quantifying sentiments expressed in various financial sources, this model not only addresses the growing demand for sophisticated data analysis tools but also paves the way for innovative applications that can enhance decision-making processes across the financial industry, promising a more informed and competitive future. Therefore, this project's objective is to develop a Financial Sentiment Analysis Model that can predict the sentiment of financial markets and assets accurately, enabling investors, traders, and financial professionals to make more informed decisions and navigate the complexities of the ever-evolving financial landscape with greater confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e5868",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d490b",
   "metadata": {},
   "source": [
    "The methodology encompasses key stages, starting with data collection from Kaggle. Subsequently, data is split, preprocessed (including sentence cleaning, tokenization, stopword removal, stemming, and lemmatization), and undergoes exploratory analysis. Imbalanced data issues are addressed, followed by label encoding. The modeling phase employs Naive Bayes with TF-IDF, Bidirectional LSTM with Word2Vec, and the RoBERTa model, concluding with comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4b975",
   "metadata": {},
   "source": [
    "## 3. Development of Financial Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4f81a",
   "metadata": {},
   "source": [
    "### 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d54fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aswin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\aswin\\anaconda3\\envs\\myenv_FSA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Operating System and File Handling\n",
    "import os\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning - Data Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Natural Language Processing (NLP) - Tokenization and Text Processing\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "nltk.download('omw-1.4')  # Downloading a specific nltk resource\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Modeling (Deep Learning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "# Machine Learning - Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Natural Language Processing (NLP) - Word Embeddings\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5ef3e",
   "metadata": {},
   "source": [
    "### 3.2 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf638611",
   "metadata": {},
   "source": [
    "This code snippet loads a CSV file named 'data.csv' from the current working directory into a Pandas DataFrame called 'data' and displays its initial rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5fc7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df: (5836, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data path\n",
    "root_folder = os.getcwd()  \n",
    "data_filename = 'data.csv' \n",
    "data_path = os.path.join(root_folder, data_filename)\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Find duplicate rows based on all columns\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "\n",
    "# Remove duplicate rows and update the DataFrame\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Print data shape and df\n",
    "print(\"Shape of df:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66f316",
   "metadata": {},
   "source": [
    "### 3.3 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57309cfc",
   "metadata": {},
   "source": [
    "This code splits the data into training, validation, and test sets with stratification, calculates their sizes as percentages of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682ee8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 5252 (90%)\n",
      "Validation data size: 292 (5%)\n",
      "Test data size: 292 (5%)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=['Sentiment'])  # Assuming you have other features besides 'Sentiment'\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Stratified split by the \"Sentiment\" label\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, random_state=7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=7)\n",
    "\n",
    "# Calculate the percentages of each subset\n",
    "total_samples = len(data)\n",
    "train_percentage = len(X_train) / total_samples * 100\n",
    "val_percentage = len(X_val) / total_samples * 100\n",
    "test_percentage = len(X_test) / total_samples * 100\n",
    "\n",
    "# Print the sizes and percentages of the subsets\n",
    "print(\"Train data size:\", len(X_train), f\"({train_percentage:.0f}%)\")\n",
    "print(\"Validation data size:\", len(X_val), f\"({val_percentage:.0f}%)\")\n",
    "print(\"Test data size:\", len(X_test), f\"({test_percentage:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4133c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create an instance of RandomOverSampler\n",
    "ros = RandomUnderSampler(sampling_strategy='auto', random_state=62)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e1610",
   "metadata": {},
   "source": [
    "### 3.4 Modeling Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e24cd",
   "metadata": {},
   "source": [
    "### A. Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17f815",
   "metadata": {},
   "source": [
    "Naive Bayes with TF-IDF is a common and effective approach for text classification tasks. It leverages TF-IDF scores to represent text data and employs the probabilistic framework of Naive Bayes for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b36fa",
   "metadata": {},
   "source": [
    "##### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524142b3",
   "metadata": {},
   "source": [
    "Text preprocessing involves cleaning and transforming text data to make it suitable for analysis or modeling. This code defines a function, preprocess_text, which preprocesses text by replacing contractions, removing URLs, special characters, and non-ASCII characters, converting to lowercase, removing emojis, tokenizing, removing common stopwords, and applying stemming. This function is applied to text data in X_train, X_val, and X_test DataFrames. The original 'Sentence' column is then dropped from each DataFrame, resulting in processed text data. Lastly, this code also label encodes the sentiment classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a58db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nb = X_train.copy()\n",
    "X_val_nb = X_val.copy()\n",
    "X_test_nb = X_test.copy()\n",
    "\n",
    "y_train_nb = y_train[:]\n",
    "y_val_nb = y_val[:]\n",
    "y_test_nb = y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b201b463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>[sale, unit, slumped, last, year, industry, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4511</th>\n",
       "      <td>[disappointment, see, plan, folded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>[nokia, 's, share, price, fell, le, one, perce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>[operating, profit, month, period, decreased, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>[operating, profit, ,, excluding, non, recurri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Processed_Tokens\n",
       "2979  [sale, unit, slumped, last, year, industry, hi...\n",
       "4511                [disappointment, see, plan, folded]\n",
       "1665  [nokia, 's, share, price, fell, le, one, perce...\n",
       "2469  [operating, profit, month, period, decreased, ...\n",
       "2037  [operating, profit, ,, excluding, non, recurri..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Clean the text\n",
    "    # Replace URLs with a space\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"http\", \" \", text)\n",
    "    # Replace '@' with 'at'\n",
    "    text = re.sub(r\"@\", \"at\", text)\n",
    "    # Replace hashtags with a space\n",
    "    text = re.sub(r\"#[A-Za-z0-9_]+\", ' ', text)\n",
    "    # Remove special characters, punctuation, and non-alphanumeric characters\n",
    "    text = re.sub(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\", \" \", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emoji characters\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([c for c in text if ord(c) < 128])\n",
    "\n",
    "    # Step 2: Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Step 3: Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    additional_stopwords = ['rt', 'mkr', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 'p', 't', 's', 'aren', 'kp', 'o', 'kat', 'de', 're', 'amp', 'will']\n",
    "    stop_words.update(additional_stopwords)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Step 4: Lemmatization with WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Step 5: Stemming with Snowball Stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "# Apply the preprocess_text function to the 'Sentence' column of X_train, X_val, and X_test\n",
    "X_train_nb['Processed_Tokens'] = X_train_nb['Sentence'].apply(preprocess_text)\n",
    "X_val_nb['Processed_Tokens'] = X_val_nb['Sentence'].apply(preprocess_text)\n",
    "X_test_nb['Processed_Tokens'] = X_test_nb['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Remove the original 'Sentence' column from X_train, X_val, and X_test\n",
    "X_train_nb.drop(columns=['Sentence'], inplace=True)\n",
    "X_val_nb.drop(columns=['Sentence'], inplace=True)\n",
    "X_test_nb.drop(columns=['Sentence'], inplace=True)\n",
    "\n",
    "# Print the processed DataFrame\n",
    "X_train_nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aedacb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels for training data\n",
    "y_train_nb = label_encoder.fit_transform(y_train_nb)\n",
    "\n",
    "# Apply preprocessing to validation and test data\n",
    "y_val_nb = label_encoder.transform(y_val_nb)\n",
    "y_test_nb = label_encoder.transform(y_test_nb)\n",
    "\n",
    "# Print the first few encoded labels for training data\n",
    "print(y_train_nb[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0ad6e",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50aa1c",
   "metadata": {},
   "source": [
    "This code performs TF-IDF vectorization on text data, creating TF-IDF representations that can be used as input features for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a83928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def tfidf_vectorize(train_set, val_set, test_set):\n",
    "    \n",
    "    # Join the tokenized words back into sentences\n",
    "    train_set['Processed_Sentences'] = train_set['Processed_Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "    val_set['Processed_Sentences'] = val_set['Processed_Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "    test_set['Processed_Sentences'] = test_set['Processed_Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Drop the Processed_Tokens column\n",
    "    train_set.drop(columns=['Processed_Tokens'], inplace=True)\n",
    "    val_set.drop(columns=['Processed_Tokens'], inplace=True)\n",
    "    test_set.drop(columns=['Processed_Tokens'], inplace=True)\n",
    "    \n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the TF-IDF vectorizer on the train set\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(train_set['Processed_Sentences'])\n",
    "    \n",
    "    # Transform the validation and test sets using the same vectorizer\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(val_set['Processed_Sentences'])\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_set['Processed_Sentences'])\n",
    "    \n",
    "    return X_train_tfidf.toarray(), X_val_tfidf.toarray(), X_test_tfidf.toarray()\n",
    "\n",
    "# Usage\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = tfidf_vectorize(X_train_nb, X_val_nb, X_test_nb)\n",
    "\n",
    "# Print the TF-IDF matrix for the first sentence on X_train\n",
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cd5d5",
   "metadata": {},
   "source": [
    "##### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067ad86",
   "metadata": {},
   "source": [
    "In this code, a Multinomial Naive Bayes classifier is optimized using RandomizedSearchCV with a predefined hyperparameter grid. The best hyperparameters are printed along with their corresponding macro F1-score, and the macro F1-scores on the training and validation sets using the best model are calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa9434d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'fit_prior': False, 'alpha': 2.0}\n",
      "Train Accuracy using Best Model: 0.85\n",
      "Validation Accuracy using Best Model: 0.60\n",
      "Train F1-score using Best Model: 0.85\n",
      "Validation F1-score using Best Model: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for random search\n",
    "param_dist = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Define the scoring metric (macro F1-score in this case)\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Create RandomizedSearchCV object with n-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=nb_model, param_distributions=param_dist, scoring=scorer, cv=5, n_iter=10, random_state=72)\n",
    "\n",
    "# Fit the random search to your data \n",
    "random_search.fit(X_train_tfidf, y_train_nb)\n",
    "\n",
    "# Print the best hyperparameters and corresponding macro F1-score\n",
    "best_hyperparameters = random_search.best_params_\n",
    "best_macro_f1_score = random_search.best_score_\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "# Calculate and print the macro F1-scores on the training and validation sets using the best model\n",
    "best_model = random_search.best_estimator_\n",
    "train_predictions = best_model.predict(X_train_tfidf)\n",
    "val_predictions = best_model.predict(X_val_tfidf)\n",
    "\n",
    "# Calculate and print the accuracy on the training and validation sets using the best model\n",
    "train_accuracy = accuracy_score(y_train_nb, train_predictions)\n",
    "val_accuracy = accuracy_score(y_val_nb, val_predictions)\n",
    "\n",
    "print(f\"Train Accuracy using Best Model: {train_accuracy:.2f}\")\n",
    "print(f\"Validation Accuracy using Best Model: {val_accuracy:.2f}\")\n",
    "\n",
    "# Calculate and print the f1 score on the training and validation sets using the best model\n",
    "train_macro_f1 = f1_score(y_train_nb, train_predictions, average='macro')\n",
    "val_macro_f1 = f1_score(y_val_nb, val_predictions, average='macro')\n",
    "\n",
    "print(f\"Train F1-score using Best Model: {train_macro_f1:.2f}\")\n",
    "print(f\"Validation F1-score using Best Model: {val_macro_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c07cb",
   "metadata": {},
   "source": [
    "### B.  Bidirectional LSTM with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa18515",
   "metadata": {},
   "source": [
    "This code trains a Word2Vec model on tokenized text data and transforms the data into Word2Vec embeddings, producing Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8319c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the DataFrames\n",
    "X_train_bi_lstm = X_train.copy()\n",
    "X_val_bi_lstm = X_val.copy()\n",
    "X_test_bi_lstm = X_test.copy()\n",
    "\n",
    "y_train_bi_lstm = y_train[:]\n",
    "y_val_bi_lstm = y_val[:]\n",
    "y_test_bi_lstm = y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6945bdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>[sale, unit, slumped, last, year, industry, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4511</th>\n",
       "      <td>[disappointment, see, plan, folded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>[nokia, share, price, fell, le, one, percent, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>[operating, profit, month, period, decreased, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>[operating, profit, ,, excluding, non, recurri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Processed_Tokens\n",
       "2979  [sale, unit, slumped, last, year, industry, hi...\n",
       "4511                [disappointment, see, plan, folded]\n",
       "1665  [nokia, share, price, fell, le, one, percent, ...\n",
       "2469  [operating, profit, month, period, decreased, ...\n",
       "2037  [operating, profit, ,, excluding, non, recurri..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Clean the text\n",
    "    # Replace URLs with a space\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"http\", \" \", text)\n",
    "    # Replace '@' with 'at'\n",
    "    text = re.sub(r\"@\", \"at\", text)\n",
    "    # Replace hashtags with a space\n",
    "    text = re.sub(r\"#[A-Za-z0-9_]+\", ' ', text)\n",
    "    # Remove special characters, punctuation, and non-alphanumeric characters\n",
    "    text = re.sub(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\", \" \", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their expanded forms\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'t\", \" not\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'m\", \" am\", text)\n",
    "\n",
    "    # Step 2: Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Step 3: Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    additional_stopwords = ['rt', 'mkr', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 'p', 't', 's', 'aren', 'kp', 'o', 'kat', 'de', 're', 'amp', 'will']\n",
    "    stop_words.update(additional_stopwords)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Step 4: Lemmatization with WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Step 5: Stemming with Snowball Stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "# Apply the preprocess_text function to the 'Sentence' column of X_train, X_val, and X_test\n",
    "X_train_bi_lstm['Processed_Tokens'] = X_train_bi_lstm['Sentence'].apply(preprocess_text)\n",
    "X_val_bi_lstm['Processed_Tokens'] = X_val_bi_lstm['Sentence'].apply(preprocess_text)\n",
    "X_test_bi_lstm['Processed_Tokens'] = X_test_bi_lstm['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Remove the original 'Sentence' column from X_train, X_val, and X_test\n",
    "X_train_bi_lstm.drop(columns=['Sentence'], inplace=True)\n",
    "X_val_bi_lstm.drop(columns=['Sentence'], inplace=True)\n",
    "X_test_bi_lstm.drop(columns=['Sentence'], inplace=True)\n",
    "\n",
    "# Print the processed DataFrame\n",
    "X_train_bi_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f6bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels for training data\n",
    "y_train_bi_lstm = label_encoder.fit_transform(y_train_bi_lstm)\n",
    "\n",
    "# Apply preprocessing to validation and test data\n",
    "y_val_bi_lstm = label_encoder.transform(y_val_bi_lstm)\n",
    "y_test_bi_lstm = label_encoder.transform(y_test_bi_lstm)\n",
    "\n",
    "# Print the first few encoded labels for training data\n",
    "print(y_train_bi_lstm[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb1fe0",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02fb31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.17835921,  0.15375838,  0.15511887,  0.15005662,  0.07745324,\n",
      "       -0.3121139 ,  0.32480076,  0.7818806 , -0.30706242, -0.27127847,\n",
      "       -0.03503095, -0.45709884, -0.16601317,  0.24844386,  0.09457346,\n",
      "       -0.3154079 ,  0.11175033, -0.39157084, -0.058618  , -0.8477492 ,\n",
      "        0.14082128,  0.21532547,  0.28559312, -0.11156128,  0.00475884,\n",
      "       -0.10021405, -0.11515267, -0.07942633, -0.48533   ,  0.01235928,\n",
      "        0.43980172,  0.06331208,  0.36226758, -0.32920563, -0.13420144,\n",
      "        0.26673862,  0.0842419 , -0.29874036, -0.15484992, -0.48903206,\n",
      "        0.17389674, -0.21252078, -0.20924143,  0.17631343,  0.26387066,\n",
      "       -0.05864932, -0.11540092, -0.00688855,  0.2586503 ,  0.25666988,\n",
      "        0.21254857, -0.22767673, -0.04937591,  0.00699232, -0.20692594,\n",
      "        0.24967958,  0.21334487, -0.13408518, -0.23693162,  0.09742279,\n",
      "       -0.11466881,  0.28453517, -0.21527539,  0.0255277 , -0.4660117 ,\n",
      "        0.43069145,  0.14200063,  0.10814288, -0.46602917,  0.31667617,\n",
      "       -0.14133102,  0.2487317 ,  0.27182308, -0.16638161,  0.29249495,\n",
      "        0.05109334,  0.18541954, -0.02673068, -0.21880805,  0.11102517,\n",
      "       -0.4011373 ,  0.03253077, -0.33952826,  0.5264052 , -0.12851572,\n",
      "       -0.02110348,  0.14106084,  0.4281767 ,  0.35593808,  0.12791131,\n",
      "        0.37681782,  0.2748135 ,  0.1056836 ,  0.04635787,  0.47092286,\n",
      "        0.37128383,  0.15913132, -0.48225996,  0.06598654, -0.00711146],\n",
      "      dtype=float32), array([-0.18107152,  0.1597276 ,  0.12572905,  0.12916192,  0.06285617,\n",
      "       -0.31789315,  0.3078444 ,  0.80624753, -0.28840715, -0.25626463,\n",
      "       -0.04126218, -0.45542878, -0.18272033,  0.24021478,  0.09907682,\n",
      "       -0.31842402,  0.10539079, -0.43479204, -0.02298007, -0.8268419 ,\n",
      "        0.14006522,  0.21347849,  0.2582839 , -0.0896542 ,  0.01546257,\n",
      "       -0.10919195, -0.14343256, -0.11935062, -0.4645257 , -0.01159592,\n",
      "        0.45428914,  0.07592547,  0.3577316 , -0.31500068, -0.13570449,\n",
      "        0.2639794 ,  0.08481086, -0.30788478, -0.13916668, -0.54807144,\n",
      "        0.15130214, -0.16321367, -0.20515114,  0.14658156,  0.24506612,\n",
      "       -0.05007502, -0.11806438,  0.01847728,  0.22526538,  0.2659148 ,\n",
      "        0.19472821, -0.2285612 , -0.07787348,  0.0265624 , -0.16690184,\n",
      "        0.22517933,  0.21733072, -0.15328006, -0.23561308,  0.08411781,\n",
      "       -0.13355702,  0.31501663, -0.2182208 ,  0.02963379, -0.4722804 ,\n",
      "        0.39505816,  0.12431367,  0.08063462, -0.47828224,  0.2934043 ,\n",
      "       -0.12912028,  0.24826777,  0.27004683, -0.17577574,  0.29476538,\n",
      "        0.03813478,  0.14915046, -0.0256655 , -0.23461267,  0.12960598,\n",
      "       -0.3671752 ,  0.03469625, -0.31528142,  0.5476744 , -0.12491448,\n",
      "       -0.00478252,  0.155136  ,  0.45509326,  0.3276793 ,  0.13228549,\n",
      "        0.37787426,  0.2960329 ,  0.10359139,  0.0602408 ,  0.48122716,\n",
      "        0.3970907 ,  0.13306272, -0.4874918 ,  0.07428911,  0.00162404],\n",
      "      dtype=float32), array([-0.00578536,  0.00092887,  0.00963219,  0.00962905,  0.00731643,\n",
      "       -0.00144271,  0.01718457,  0.02425274, -0.01354819, -0.01290304,\n",
      "       -0.00054592, -0.01981854, -0.0040793 ,  0.00093022,  0.0029033 ,\n",
      "       -0.00781868,  0.01092197, -0.01332163,  0.00154771, -0.0284634 ,\n",
      "        0.00162062,  0.00271806,  0.00450789, -0.00748598, -0.00716287,\n",
      "        0.00595513,  0.00292806, -0.00574643, -0.01003072,  0.00298609,\n",
      "        0.01127191,  0.01165303,  0.003137  , -0.01464962, -0.0103543 ,\n",
      "        0.00551799,  0.01019846, -0.01661049, -0.00717158, -0.00298708,\n",
      "        0.00378216, -0.00842496, -0.01238647,  0.00998663,  0.0056079 ,\n",
      "        0.00771399,  0.0003718 , -0.00779035, -0.00450568,  0.011837  ,\n",
      "        0.0046989 , -0.00401737, -0.00559309,  0.00806358, -0.01095096,\n",
      "       -0.00432775,  0.00884179, -0.01292035,  0.00352337, -0.0068813 ,\n",
      "        0.005289  ,  0.01328581,  0.00114136,  0.00490012, -0.00893541,\n",
      "        0.01362918,  0.00278097,  0.01152019, -0.0150376 ,  0.00252343,\n",
      "       -0.00100595, -0.00059037, -0.00183429, -0.01176888,  0.00965643,\n",
      "        0.00794186, -0.00586023,  0.00303485, -0.00468238, -0.00144186,\n",
      "       -0.00118047, -0.00341883, -0.00269418,  0.01753047, -0.00093106,\n",
      "       -0.00584418,  0.00920325,  0.01895308,  0.01299489,  0.00043945,\n",
      "        0.00010503,  0.01639507,  0.00955504,  0.00357094,  0.02021494,\n",
      "        0.01120118,  0.0101361 , -0.01165948, -0.00607451, -0.00276173],\n",
      "      dtype=float32), array([-0.1078409 ,  0.08396998,  0.08913789,  0.08020019,  0.04452781,\n",
      "       -0.17556696,  0.19231999,  0.449484  , -0.17570189, -0.15670276,\n",
      "       -0.02201054, -0.27363524, -0.10511998,  0.1322055 ,  0.06585546,\n",
      "       -0.18428308,  0.04763884, -0.24400896, -0.02689181, -0.4878126 ,\n",
      "        0.08363911,  0.12767673,  0.15562817, -0.04492003,  0.00863306,\n",
      "       -0.06835388, -0.07135053, -0.0608192 , -0.27232847, -0.01359191,\n",
      "        0.25272837,  0.03665879,  0.21588373, -0.18617043, -0.08255811,\n",
      "        0.1558761 ,  0.04868241, -0.1683694 , -0.08507444, -0.29372406,\n",
      "        0.1021112 , -0.12262065, -0.11005843,  0.09961016,  0.14153108,\n",
      "       -0.02275831, -0.07173357, -0.00121415,  0.12522785,  0.15807831,\n",
      "        0.10860875, -0.13238044, -0.04034329,  0.00440667, -0.09627826,\n",
      "        0.12736565,  0.11805169, -0.07361239, -0.14315271,  0.06068646,\n",
      "       -0.07869903,  0.17781965, -0.12230189,  0.00773895, -0.28192535,\n",
      "        0.22746187,  0.08951803,  0.06259953, -0.2696604 ,  0.18704236,\n",
      "       -0.07806917,  0.14799803,  0.15620317, -0.10633048,  0.1824002 ,\n",
      "        0.01723304,  0.08949042, -0.01207385, -0.12787455,  0.06563834,\n",
      "       -0.21325138,  0.02611073, -0.1941378 ,  0.31827903, -0.07583908,\n",
      "        0.00449375,  0.07358201,  0.25511828,  0.20289707,  0.06556112,\n",
      "        0.214114  ,  0.15708613,  0.07072178,  0.03593198,  0.26561028,\n",
      "        0.22355683,  0.08435086, -0.2860993 ,  0.04022564, -0.00948142],\n",
      "      dtype=float32), array([-0.18415734,  0.15597066,  0.13641158,  0.1409151 ,  0.08131534,\n",
      "       -0.27501497,  0.30363694,  0.69647384, -0.28131533, -0.23439924,\n",
      "       -0.02305283, -0.42025384, -0.14109989,  0.21959074,  0.0959734 ,\n",
      "       -0.28842124,  0.08358862, -0.35513747, -0.05501666, -0.7555422 ,\n",
      "        0.13051811,  0.19663335,  0.27063894, -0.10212037,  0.02079482,\n",
      "       -0.09686215, -0.10196048, -0.09102689, -0.42114332, -0.00959375,\n",
      "        0.41464034,  0.04274269,  0.33245268, -0.27123985, -0.11928504,\n",
      "        0.2513469 ,  0.07846042, -0.25616303, -0.12522325, -0.45901513,\n",
      "        0.14677703, -0.19048046, -0.17952286,  0.15415412,  0.22218409,\n",
      "       -0.04457645, -0.0923628 , -0.00266568,  0.21073018,  0.22677422,\n",
      "        0.20459995, -0.19899926, -0.05240279,  0.00791609, -0.17604229,\n",
      "        0.21051742,  0.20844226, -0.12025071, -0.21021451,  0.07017473,\n",
      "       -0.12241962,  0.25328356, -0.1803724 ,  0.01142205, -0.43448618,\n",
      "        0.3765086 ,  0.13409998,  0.09585334, -0.42640242,  0.2935033 ,\n",
      "       -0.14065085,  0.22425352,  0.25051412, -0.1300051 ,  0.259495  ,\n",
      "        0.03531933,  0.14591007, -0.02816525, -0.20898144,  0.11284383,\n",
      "       -0.3484653 ,  0.02400548, -0.29001743,  0.46605995, -0.10518209,\n",
      "       -0.01390379,  0.11754347,  0.35989037,  0.31156796,  0.12621899,\n",
      "        0.331559  ,  0.24968748,  0.10022961,  0.02936055,  0.43582043,\n",
      "        0.35132092,  0.14697881, -0.41896522,  0.06427385, -0.01404264],\n",
      "      dtype=float32), array([-0.11117749,  0.10398614,  0.09671197,  0.07451688,  0.0385552 ,\n",
      "       -0.22819868,  0.21626867,  0.5533108 , -0.18229792, -0.1750836 ,\n",
      "       -0.02229282, -0.31704533, -0.1240319 ,  0.15812342,  0.06185984,\n",
      "       -0.21440665,  0.06601274, -0.29731384, -0.01034837, -0.55865484,\n",
      "        0.1004543 ,  0.14957626,  0.17359053, -0.0659449 ,  0.00563146,\n",
      "       -0.06637028, -0.1025323 , -0.07670968, -0.3077907 , -0.01106558,\n",
      "        0.3080895 ,  0.05651243,  0.2505885 , -0.20474428, -0.0956293 ,\n",
      "        0.17560913,  0.06385698, -0.21227378, -0.09059941, -0.3783186 ,\n",
      "        0.11103076, -0.11863   , -0.13605434,  0.10346062,  0.17739153,\n",
      "       -0.03551594, -0.07856506,  0.02227642,  0.15499982,  0.17942256,\n",
      "        0.1254649 , -0.16285177, -0.05634575,  0.00690561, -0.11518239,\n",
      "        0.15518989,  0.1548301 , -0.11914133, -0.16019471,  0.06297116,\n",
      "       -0.08419006,  0.20840752, -0.13869755,  0.01992383, -0.32546613,\n",
      "        0.26939783,  0.09418178,  0.05732082, -0.32682794,  0.21129549,\n",
      "       -0.09722254,  0.16120772,  0.17520618, -0.11340999,  0.19851966,\n",
      "        0.02166494,  0.10635144, -0.02236559, -0.17231652,  0.08339813,\n",
      "       -0.24960257,  0.03014819, -0.21863522,  0.39331862, -0.06992702,\n",
      "       -0.00763159,  0.11339532,  0.31420544,  0.22220121,  0.085343  ,\n",
      "        0.2597419 ,  0.19721597,  0.08090418,  0.04160588,  0.33976525,\n",
      "        0.26925075,  0.08574326, -0.3438955 ,  0.04652718,  0.00136536],\n",
      "      dtype=float32), array([-7.93672055e-02,  6.94190785e-02,  6.10899851e-02,  5.62382191e-02,\n",
      "        3.26091237e-02, -1.34180307e-01,  1.39774501e-01,  3.48512232e-01,\n",
      "       -1.30207047e-01, -1.04332149e-01, -1.73245817e-02, -1.99466914e-01,\n",
      "       -8.86344984e-02,  1.05903327e-01,  3.55049260e-02, -1.24776222e-01,\n",
      "        5.23018129e-02, -1.73707679e-01, -1.16782123e-02, -3.52041870e-01,\n",
      "        5.26814759e-02,  1.01124063e-01,  1.18994340e-01, -4.66177166e-02,\n",
      "        9.72146401e-04, -3.73389907e-02, -5.70946112e-02, -5.27735762e-02,\n",
      "       -1.90962151e-01, -1.16844727e-02,  1.96042761e-01,  3.55108418e-02,\n",
      "        1.59169003e-01, -1.24053642e-01, -5.63807562e-02,  1.07240684e-01,\n",
      "        3.85063551e-02, -1.31865457e-01, -6.74686581e-02, -2.23914608e-01,\n",
      "        6.88130930e-02, -7.47599676e-02, -8.85426924e-02,  6.36010915e-02,\n",
      "        1.01919055e-01, -2.12309640e-02, -5.14199510e-02,  4.91196988e-03,\n",
      "        9.42144468e-02,  1.21110544e-01,  9.15733427e-02, -9.34571996e-02,\n",
      "       -2.90006921e-02,  7.13350577e-03, -7.96760544e-02,  1.05940998e-01,\n",
      "        9.81928557e-02, -7.65744746e-02, -8.92943069e-02,  4.25942764e-02,\n",
      "       -4.80915979e-02,  1.22339532e-01, -8.82837027e-02,  1.90703000e-03,\n",
      "       -2.04294279e-01,  1.63335964e-01,  6.21552952e-02,  2.91902889e-02,\n",
      "       -2.08621711e-01,  1.20272726e-01, -6.00906685e-02,  1.00736402e-01,\n",
      "        1.10934757e-01, -7.65397176e-02,  1.24887809e-01,  2.63643544e-02,\n",
      "        6.10596463e-02, -3.72934956e-05, -1.08891882e-01,  6.08512498e-02,\n",
      "       -1.56066880e-01,  1.22973304e-02, -1.34282574e-01,  2.38190070e-01,\n",
      "       -5.41131794e-02,  3.63152800e-03,  7.13826939e-02,  1.89262420e-01,\n",
      "        1.36062145e-01,  5.94042949e-02,  1.68879941e-01,  1.23824343e-01,\n",
      "        5.61033040e-02,  1.45167755e-02,  2.13216662e-01,  1.67545497e-01,\n",
      "        6.27948418e-02, -2.01811999e-01,  2.56288275e-02,  7.71408761e-03],\n",
      "      dtype=float32), array([-0.01545635,  0.01553189,  0.00455834,  0.00467641,  0.00146927,\n",
      "       -0.02891709,  0.02425854,  0.0437896 , -0.02462666, -0.01773119,\n",
      "       -0.00866133, -0.02607154, -0.00970876,  0.01697768,  0.00754437,\n",
      "       -0.01505839,  0.00227167, -0.02010738, -0.00532389, -0.04904594,\n",
      "        0.00812215,  0.02202358,  0.01320984, -0.01039519,  0.00170103,\n",
      "       -0.00986298, -0.0124387 , -0.0057433 , -0.03540906, -0.00553764,\n",
      "        0.03801551,  0.01475269,  0.01683588, -0.02556121, -0.0137645 ,\n",
      "        0.02684098,  0.01355132, -0.00993235, -0.01095205, -0.031585  ,\n",
      "        0.00097571, -0.01245977, -0.00925324,  0.01673291,  0.02199675,\n",
      "       -0.01048274, -0.00324787, -0.00687403,  0.01124755,  0.02177042,\n",
      "        0.00853543, -0.01112299, -0.00669876,  0.00146172, -0.01424547,\n",
      "        0.00941551,  0.02338054, -0.00673384, -0.0207475 ,  0.00251608,\n",
      "       -0.00603395,  0.01410978, -0.01782655,  0.00984111, -0.03688246,\n",
      "        0.02568727,  0.01459532, -0.00296866, -0.03026761,  0.01938052,\n",
      "       -0.00142469,  0.02033105,  0.0241886 , -0.02043678,  0.01138961,\n",
      "        0.0123907 ,  0.01932944, -0.00176127, -0.0104319 ,  0.00962061,\n",
      "       -0.01989012,  0.00513744, -0.01886108,  0.02811336, -0.01354332,\n",
      "       -0.00602242,  0.01262199,  0.0325016 ,  0.02983035,  0.00113483,\n",
      "        0.01662027,  0.02460464,  0.00883391, -0.00123219,  0.02917682,\n",
      "        0.02843089,  0.00844599, -0.03123481,  0.0002926 ,  0.00949248],\n",
      "      dtype=float32), array([-1.0330312e-02,  1.4578216e-03,  4.4149049e-03,  1.0888237e-02,\n",
      "       -4.9412912e-03, -1.1577408e-02,  1.9398836e-02,  3.9665118e-02,\n",
      "       -1.5515167e-02, -1.7450118e-02, -5.1150522e-03, -1.4638321e-02,\n",
      "       -1.1537739e-02,  5.5740988e-03, -2.0420779e-03, -7.6267575e-03,\n",
      "        3.3734410e-04, -1.6695760e-02, -1.0595433e-02, -4.4104438e-02,\n",
      "        8.7562902e-04,  1.8471761e-02,  1.5459016e-02, -5.7943701e-03,\n",
      "        5.0108605e-03,  8.0010080e-04, -7.9330299e-03,  4.5435780e-04,\n",
      "       -1.6479425e-02, -6.2827221e-03,  1.8034503e-02,  9.1667119e-03,\n",
      "        1.1603178e-02, -6.8408018e-03, -7.8895306e-03,  1.2882384e-02,\n",
      "        1.4995472e-03, -1.5390719e-02, -9.8660141e-03, -2.1199599e-02,\n",
      "       -1.5500729e-03, -1.4095730e-02, -7.8247664e-03,  7.4307220e-03,\n",
      "        2.0946898e-03, -1.3337461e-03, -3.9608232e-04,  9.4128074e-04,\n",
      "        1.4540447e-03,  1.9752381e-02,  1.2254145e-02, -1.8244913e-02,\n",
      "       -1.0510746e-02, -5.3618457e-03, -1.3102735e-02, -1.5066471e-05,\n",
      "        1.3385138e-02, -1.2970770e-02, -7.9215476e-03, -4.7208229e-03,\n",
      "        3.0281727e-04,  1.8920483e-02, -1.6374692e-03, -6.2118783e-03,\n",
      "       -1.7651582e-02,  2.2799585e-02,  1.4920245e-02,  1.3390620e-03,\n",
      "       -1.6554499e-02,  1.6406348e-02, -8.5168695e-03,  5.5163982e-04,\n",
      "        1.8457133e-02, -1.4432935e-02,  2.1774692e-02,  7.2293468e-03,\n",
      "        9.4685797e-03, -1.0719244e-02, -4.2304639e-03,  2.1500343e-03,\n",
      "       -8.3817896e-03, -7.9525439e-03, -1.8791394e-02,  2.6477909e-02,\n",
      "        3.2508846e-03, -6.3463417e-03,  1.1050150e-02,  1.9800052e-02,\n",
      "        3.3403218e-03, -3.6545885e-03,  8.7658707e-03,  1.9081380e-02,\n",
      "       -2.7200135e-03,  5.9416718e-03,  1.7073425e-02,  2.3081876e-02,\n",
      "        1.0069730e-03, -2.0679142e-02, -7.9313817e-05, -6.9627911e-03],\n",
      "      dtype=float32), array([-0.07805689,  0.06723193,  0.06352516,  0.04497784,  0.02629087,\n",
      "       -0.14139783,  0.13071859,  0.3294224 , -0.11541887, -0.10759088,\n",
      "       -0.02300819, -0.19827673, -0.08388779,  0.1066511 ,  0.04335798,\n",
      "       -0.13289875,  0.04800874, -0.18563414, -0.01843196, -0.33716732,\n",
      "        0.06909137,  0.09149088,  0.10440277, -0.04465192,  0.01391743,\n",
      "       -0.04847486, -0.06020085, -0.04023699, -0.1904573 ,  0.00385598,\n",
      "        0.18388428,  0.03767078,  0.14568782, -0.13603303, -0.06566913,\n",
      "        0.10447805,  0.03321387, -0.13415088, -0.05073917, -0.22828706,\n",
      "        0.05877637, -0.06848312, -0.07574116,  0.05467867,  0.10572381,\n",
      "       -0.03352856, -0.05984127,  0.01207511,  0.09828663,  0.10958306,\n",
      "        0.06939045, -0.08706488, -0.03882323,  0.01382094, -0.06659079,\n",
      "        0.08409035,  0.08548862, -0.05836956, -0.08599433,  0.03701907,\n",
      "       -0.05614303,  0.1355238 , -0.09305   ,  0.00639321, -0.19301926,\n",
      "        0.15674025,  0.06248919,  0.04390449, -0.20194522,  0.1168009 ,\n",
      "       -0.05610414,  0.09963096,  0.09880558, -0.0750403 ,  0.13173236,\n",
      "        0.01400604,  0.05627435, -0.00069601, -0.09947103,  0.05763779,\n",
      "       -0.15716185,  0.00587295, -0.12401751,  0.24050112, -0.05858815,\n",
      "        0.00250245,  0.05633594,  0.18140727,  0.12426793,  0.04771363,\n",
      "        0.15502281,  0.12318639,  0.04298748,  0.02684271,  0.1919999 ,\n",
      "        0.15854193,  0.0565416 , -0.19764957,  0.03194639,  0.00139501],\n",
      "      dtype=float32), array([-0.01295844,  0.01526178,  0.01429209,  0.00513709,  0.00671748,\n",
      "       -0.02587367,  0.01969338,  0.05590894, -0.01928229, -0.00686835,\n",
      "       -0.00316096, -0.03139185, -0.00484521,  0.00429718,  0.01086529,\n",
      "       -0.0104776 , -0.00250082, -0.02514119, -0.00383694, -0.0467266 ,\n",
      "        0.00010126,  0.01968024,  0.02271408,  0.00467727,  0.00475383,\n",
      "        0.0025099 , -0.01241774, -0.00554949, -0.03464097, -0.00981645,\n",
      "        0.02824653, -0.00269023,  0.0180035 , -0.01353178, -0.00967956,\n",
      "        0.01301201,  0.01284555, -0.01293394, -0.00075576, -0.03979115,\n",
      "        0.0099523 , -0.00879117, -0.00769356,  0.00979575,  0.00538413,\n",
      "       -0.00349675, -0.01607963, -0.00722392,  0.00557248,  0.01335913,\n",
      "        0.01731637, -0.00543234,  0.0032602 , -0.00206075, -0.00141628,\n",
      "        0.00407213,  0.0094441 , -0.00305527, -0.00533918,  0.0118081 ,\n",
      "       -0.01668473,  0.02539095, -0.00578534,  0.00584578, -0.0265844 ,\n",
      "        0.02827348, -0.00134674, -0.00261727, -0.02677779,  0.01387151,\n",
      "        0.00186213,  0.01798721,  0.00695495, -0.0152223 ,  0.01644538,\n",
      "        0.00151004,  0.00925957,  0.00626301, -0.0095557 ,  0.00050703,\n",
      "       -0.01733147, -0.00365615, -0.00913363,  0.04202323, -0.01521495,\n",
      "        0.00358564,  0.013888  ,  0.02625443,  0.01923017,  0.00976017,\n",
      "        0.02502945,  0.0253052 ,  0.01134437, -0.00413458,  0.03663021,\n",
      "        0.02073328,  0.00758223, -0.03195609,  0.01184909, -0.00308172],\n",
      "      dtype=float32), array([-1.64320264e-02,  3.17283603e-03,  2.93889688e-03,  6.04538727e-05,\n",
      "        5.37284324e-03, -1.86826773e-02,  1.16712395e-02,  4.21650372e-02,\n",
      "       -1.60914995e-02, -1.18009150e-02,  4.31927573e-03, -1.37149803e-02,\n",
      "        4.95598710e-04,  9.38652363e-03, -5.47624100e-03, -1.40658775e-02,\n",
      "        1.67329947e-03, -9.18701198e-03,  3.52342823e-03, -3.73709649e-02,\n",
      "        8.43294431e-03,  1.03897788e-02,  1.36364540e-02,  2.47224187e-03,\n",
      "       -4.70806612e-03, -1.33508444e-03, -5.51192695e-03, -6.26048259e-03,\n",
      "       -2.74125598e-02,  4.79061482e-03,  1.71040576e-02,  1.10317888e-02,\n",
      "        2.39257906e-02, -6.79914188e-03, -6.43775845e-03,  1.67597346e-02,\n",
      "        4.14040778e-03, -1.94299519e-02, -2.33101798e-03, -3.02572586e-02,\n",
      "        5.63057372e-03,  3.62302002e-04, -8.52566026e-03, -1.94569945e-03,\n",
      "        1.81318410e-02, -1.10594807e-02, -6.76754676e-03,  9.54430364e-03,\n",
      "        7.75952963e-03,  1.93723571e-02,  1.66260656e-02, -1.74577739e-02,\n",
      "       -3.25507578e-03, -3.82552011e-04, -1.45653049e-02,  1.26085314e-03,\n",
      "        9.96425562e-03, -7.31331250e-03, -1.43892122e-02,  9.41272452e-03,\n",
      "        1.31517462e-03,  1.96605157e-02, -1.50330539e-03, -4.97444579e-03,\n",
      "       -1.79793779e-02,  8.98680277e-03,  3.51543771e-03,  1.23247560e-02,\n",
      "       -2.90323645e-02,  3.72490473e-03, -4.32730746e-03,  1.55551787e-02,\n",
      "        5.52102877e-03, -1.07239010e-02,  1.49992174e-02,  4.91536409e-03,\n",
      "        2.38803239e-03, -6.10942813e-03, -1.85525615e-03,  6.28592633e-03,\n",
      "       -2.19844729e-02, -8.69217701e-03, -8.91396776e-03,  1.63096599e-02,\n",
      "        4.49289056e-03,  5.93641819e-03,  1.57437362e-02,  1.76192001e-02,\n",
      "        4.09558415e-03,  1.11344028e-02,  1.90452654e-02,  5.73565857e-03,\n",
      "       -3.38647980e-03,  1.89138623e-03,  1.82912536e-02,  1.19597521e-02,\n",
      "        4.76171682e-03, -2.37451345e-02,  2.47608963e-03, -2.72783358e-03],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "def train_word2vec_model(df):\n",
    "    # Ensure that the 'Processed_Tokens' column contains lists of tokens\n",
    "    sentences = df['Processed_Tokens'].tolist()\n",
    "    \n",
    "    # Create and train the Word2Vec model\n",
    "    model = Word2Vec(sentences, \n",
    "                     vector_size=100,  # You can adjust the vector size as needed\n",
    "                     window=3,          # Context window size\n",
    "                     min_count=2,       # Minimum word frequency\n",
    "                     sg=0,              # CBOW model (sg=1 for Skip-gram)\n",
    "                     workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def transform_to_word2vec(df, model):\n",
    "    # Tokenize sentences\n",
    "    sentences = df['Processed_Tokens'].tolist()\n",
    "    \n",
    "    # Transform sentences to Word2Vec vectors\n",
    "    transformed_data = []\n",
    "    for sentence in sentences:\n",
    "        word_vectors = []\n",
    "        for word in sentence:\n",
    "            if word in model.wv:\n",
    "                word_vectors.append(model.wv[word])\n",
    "        transformed_data.append(word_vectors)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "# Combine train, validation, and test sets into one DataFrame\n",
    "combined_df = pd.concat([X_train_bi_lstm, X_val_bi_lstm, X_test_bi_lstm], ignore_index=True)\n",
    "\n",
    "# Train the Word2Vec model on the combined data\n",
    "word2vec_model = train_word2vec_model(combined_df)\n",
    "\n",
    "# Transform the data using the trained model\n",
    "X_train_word2vec = transform_to_word2vec(X_train_bi_lstm, word2vec_model)\n",
    "X_val_word2vec = transform_to_word2vec(X_val_bi_lstm, word2vec_model)\n",
    "X_test_word2vec = transform_to_word2vec(X_test_bi_lstm, word2vec_model)\n",
    "\n",
    "# Print the word2vec embedding for first row\n",
    "print(X_train_word2vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fcbf86",
   "metadata": {},
   "source": [
    "##### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60df141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences_custom(train_data, val_data, test_data):\n",
    "    # Determine the maximum sequence length for each dataset\n",
    "    max_sequence_length_train = max(len(seq) for seq in train_data)\n",
    "    max_sequence_length_val = max(len(seq) for seq in val_data)\n",
    "    max_sequence_length_test = max(len(seq) for seq in test_data)\n",
    "    \n",
    "    # Calculate the overall maximum sequence length\n",
    "    max_sequence_length = max(max_sequence_length_train, max_sequence_length_val, max_sequence_length_test)\n",
    "    \n",
    "    # Pad sequences to match the calculated maximum sequence length\n",
    "    train_padded = pad_sequences(train_data, maxlen=max_sequence_length)\n",
    "    val_padded = pad_sequences(val_data, maxlen=max_sequence_length)\n",
    "    test_padded = pad_sequences(test_data, maxlen=max_sequence_length)\n",
    "    \n",
    "    return train_padded, val_padded, test_padded, max_sequence_length\n",
    "\n",
    "# Usage\n",
    "X_train_padded, X_val_padded, X_test_padded, max_sequence_length = pad_sequences_custom(X_train_word2vec, X_val_word2vec, X_test_word2vec)\n",
    "\n",
    "# Print padded sentence for the first row on X_train\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071c48a",
   "metadata": {},
   "source": [
    "##### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469102c",
   "metadata": {},
   "source": [
    "This code reshapes the data into a 3D format, adding a dimension, for compatibility with deep learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ce612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_reshaped: (2358, 54, 100)\n",
      "Shape of X_val_reshaped: (292, 54, 100)\n",
      "Shape of X_test_reshaped: (292, 54, 100)\n"
     ]
    }
   ],
   "source": [
    "def reshape_for_deep_learning(data):\n",
    "    # Determine the maximum number of words in a sentence\n",
    "    max_len = max(len(sentence) for sentence in data)\n",
    "    \n",
    "    # Create a 3D array to hold the data\n",
    "    num_samples = len(data)\n",
    "    embedding_dim = len(data[0][0])  # Assuming all embeddings have the same dimension\n",
    "    reshaped_data = np.zeros((num_samples, max_len, embedding_dim))\n",
    "    \n",
    "    # Fill the 3D array with data\n",
    "    for i, sentence in enumerate(data):\n",
    "        for j, word_vector in enumerate(sentence):\n",
    "            reshaped_data[i, j, :] = word_vector\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "# Reshape the data for deep learning models\n",
    "X_train_reshaped = reshape_for_deep_learning(X_train_padded)\n",
    "X_val_reshaped = reshape_for_deep_learning(X_val_padded)\n",
    "X_test_reshaped = reshape_for_deep_learning(X_test_padded)\n",
    "\n",
    "# Print the shape of the reshaped data to verify\n",
    "print(\"Shape of X_train_reshaped:\", X_train_reshaped.shape)\n",
    "print(\"Shape of X_val_reshaped:\", X_val_reshaped.shape)\n",
    "print(\"Shape of X_test_reshaped:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7100216",
   "metadata": {},
   "source": [
    "##### Bidirectional LSTM Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd9254",
   "metadata": {},
   "source": [
    "This code defines and trains a Bidirectional LSTM neural network model. It uses F1 metric, a specific learning rate, and early stopping to prevent overfitting during training. The training history is stored in the 'history' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "786/786 [==============================] - 66s 74ms/step - loss: 1.0987 - accuracy: 0.3448 - f1_m: 0.0013 - val_loss: 1.0724 - val_accuracy: 0.4932 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/30\n",
      "786/786 [==============================] - 56s 72ms/step - loss: 1.0903 - accuracy: 0.3719 - f1_m: 0.0279 - val_loss: 1.1969 - val_accuracy: 0.1849 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/30\n",
      "786/786 [==============================] - 57s 72ms/step - loss: 1.0908 - accuracy: 0.3723 - f1_m: 0.0327 - val_loss: 1.0589 - val_accuracy: 0.3527 - val_f1_m: 0.0429\n",
      "Epoch 4/30\n",
      "240/786 [========>.....................] - ETA: 38s - loss: 1.0888 - accuracy: 0.3569 - f1_m: 0.0350"
     ]
    }
   ],
   "source": [
    "# Set a specific random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Determine the number of unique classes (labels)\n",
    "num_classes = len(np.unique(y_train_bi_lstm))\n",
    "\n",
    "# Create an LSTM model with fewer layers and simpler architecture\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, kernel_initializer='glorot_uniform')))\n",
    "bi_lstm_model.add(Dense(128, activation='relu'))\n",
    "bi_lstm_model.add(Dense(64, activation='relu'))\n",
    "bi_lstm_model.add(Dense(32, activation='relu'))\n",
    "bi_lstm_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Define custom metrics\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Create an optimizer with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and custom metrics\n",
    "bi_lstm_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_m])\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = bi_lstm_model.fit(\n",
    "    X_train_reshaped,\n",
    "    tf.keras.utils.to_categorical(y_train_bi_lstm, num_classes=num_classes),\n",
    "    epochs=30,\n",
    "    batch_size=3,\n",
    "    validation_data=(X_val_reshaped, tf.keras.utils.to_categorical(y_val_bi_lstm)),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f53a8a",
   "metadata": {},
   "source": [
    "#### 3.8.3  RoBERTa Model with RoBERTa preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e70c7",
   "metadata": {},
   "source": [
    "RoBERTa model with RoBERTa preprocessing involves using the RoBERTa tokenizer and pre-trained RoBERTa model for sentiment analysis. The key idea is to leverage the power of pre-trained RoBERTa models for various NLP tasks by fine-tuning or using them as feature extractors. The RoBERTa preprocessing ensures that our text data is compatible with the RoBERTa model's input format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea28692",
   "metadata": {},
   "source": [
    "##### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88120ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the DataFrames\n",
    "X_train_roberta = X_train.copy()\n",
    "X_val_roberta = X_val.copy()\n",
    "X_test_roberta = X_test.copy()\n",
    "\n",
    "y_train_roberta = y_train[:]\n",
    "y_val_roberta = y_val[:]\n",
    "y_test_roberta = y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Clean the text\n",
    "    # Replace contractions with their expanded forms\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'t\", \" not\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'m\", \" am\", text)\n",
    "\n",
    "    # Replace URLs with a space\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"http\", \" \", text)\n",
    "    # Replace '@' with 'at'\n",
    "    text = re.sub(r\"@\", \"at\", text)\n",
    "    # Replace hashtags with a space\n",
    "    text = re.sub(r\"#[A-Za-z0-9_]+\", ' ', text)\n",
    "    # Remove special characters, punctuation, and non-alphanumeric characters\n",
    "    text = re.sub(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\", \" \", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emoji characters\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([c for c in text if ord(c) < 128])\n",
    "\n",
    "    # Step 2: Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    additional_stopwords = ['rt', 'mkr', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 'p', 't', 's', 'aren', 'kp', 'o', 'kat', 'de', 're', 'amp', 'will']\n",
    "    stop_words.update(additional_stopwords)\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    cleaned_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'Sentence' column of X_train, X_val, and X_test\n",
    "X_train_roberta['Processed_Tokens'] = X_train_roberta['Sentence'].apply(preprocess_text)\n",
    "X_val_roberta['Processed_Tokens'] = X_val_roberta['Sentence'].apply(preprocess_text)\n",
    "X_test_roberta['Processed_Tokens'] = X_test_roberta['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Remove the original 'Sentence' column from X_train, X_val, and X_test\n",
    "X_train_roberta.drop(columns=['Sentence'], inplace=True)\n",
    "X_val_roberta.drop(columns=['Sentence'], inplace=True)\n",
    "X_test_roberta.drop(columns=['Sentence'], inplace=True)\n",
    "\n",
    "# Print the processed DataFrame\n",
    "X_train_roberta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels for training data\n",
    "y_train_roberta = label_encoder.fit_transform(y_train_roberta)\n",
    "\n",
    "# Apply preprocessing to validation and test data\n",
    "y_val_roberta = label_encoder.transform(y_val_roberta)\n",
    "y_test_roberta = label_encoder.transform(y_test_roberta)\n",
    "\n",
    "# Print the first few encoded labels for training data\n",
    "print(y_train_roberta[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e6edf",
   "metadata": {},
   "source": [
    "##### RoBERTa Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcc36b",
   "metadata": {},
   "source": [
    "These steps prepare our data for passing as input to the RoBERTa model, enabling the train and evaluation stages of the model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa tokenizer\n",
    "model_name = \"roberta-base\"  # Choose the appropriate variant\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization and preprocessing function\n",
    "def preprocess_data(X, y, max_seq_length=128):\n",
    "    # Flatten X\n",
    "    X = X.squeeze()\n",
    "\n",
    "    # Join tokenized words into a single string sentence\n",
    "    tokenized_sentences = [\" \".join(sentence) for sentence in X]\n",
    "\n",
    "    # Tokenize and pad the sentences\n",
    "    tokenized_sentences = [tokenizer.encode(sentence, max_length=max_seq_length, pad_to_max_length=True) for sentence in tokenized_sentences]\n",
    "\n",
    "    # Convert labels to categorical if needed\n",
    "    num_classes = len(np.unique(y))\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "    return tokenized_sentences, y\n",
    "\n",
    "# Preprocess the training data\n",
    "X_train_processed, y_train_processed = preprocess_data(X_train_roberta, y_train_roberta)\n",
    "\n",
    "# Create a TensorFlow Dataset for training data\n",
    "batch_size = 32  # Adjust this based on your preference\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_processed, y_train_processed))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=-1)\n",
    "\n",
    "# Preprocess the validation data\n",
    "X_val_processed, y_val_processed = preprocess_data(X_val_roberta, y_val_roberta)\n",
    "\n",
    "# Create a TensorFlow Dataset for validation data\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_processed, y_val_processed))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=-1)\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test_processed, y_test_processed = preprocess_data(X_test_roberta, y_test_roberta)\n",
    "\n",
    "# Create a TensorFlow Dataset for test data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_processed, y_test_processed))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0dc252",
   "metadata": {},
   "source": [
    "##### RoBERTa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb528b6f",
   "metadata": {},
   "source": [
    "This code performs fine-tuning of a RoBERTa model for sentiment prediction and evaluates its performance on a validation dataset. It uses F1 metric and the training history is stored in the 'history' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune some layers within the RoBERTa model\n",
    "for layer in model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add dropout and regularization\n",
    "classifier = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "# Combine the RoBERTa model and the classifier\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
    "outputs = model(input_ids)[0]  # Output from RoBERTa\n",
    "predictions = classifier(outputs)\n",
    "\n",
    "# Define custom metrics\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "# Create the combined model\n",
    "classification_model = tf.keras.Model(inputs=input_ids, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "classification_model.compile(optimizer='adam',\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy', f1_m])\n",
    "\n",
    "# Train the model on the training data\n",
    "history = classification_model.fit(train_dataset, validation_data=val_dataset, epochs=5)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "score = classification_model.evaluate(val_dataset)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation Loss:\", score[0])\n",
    "print(\"Validation Accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_FSA",
   "language": "python",
   "name": "myenv_fsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
